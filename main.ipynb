{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e055392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\n",
    "\n",
    "planets = NasaExoplanetArchive.query_criteria(\n",
    "    table=\"pscomppars\",\n",
    "    select=\"hostname, discoverymethod, disc_facility\",\n",
    "    where=\"discoverymethod like 'Transit'\"\n",
    ").to_pandas()\n",
    "\n",
    "\n",
    "planet_targets = planets[\n",
    "    planets['disc_facility'].str.contains('TESS', na=False)\n",
    "]['hostname'].unique().tolist()\n",
    "\n",
    "print(len(planet_targets), \"planet hosts found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d51e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroquery.mast import Catalogs\n",
    "import random\n",
    "\n",
    "try:\n",
    "    tic_query = Catalogs.query_region(\n",
    "        \"00h42m44s +41d16m09s\",  # Andromeda region (well-observed by TESS)\n",
    "        radius=5,  \n",
    "        catalog=\"TIC\"\n",
    "    )\n",
    "    \n",
    "    mask = (tic_query['Tmag'] > 8) & (tic_query['Tmag'] < 13)\n",
    "    filtered_tic = tic_query[mask]\n",
    "    \n",
    "    all_tic_targets = [f\"TIC {int(tic)}\" for tic in filtered_tic['ID'][:600]]\n",
    "    \n",
    "    control_stars = [t for t in all_tic_targets if t not in planet_targets]\n",
    "    \n",
    "    random.seed(42)\n",
    "    random.shuffle(control_stars)\n",
    "    \n",
    "    print(f\"âœ“ {len(control_stars)} control stars available (will collect until 150+ successful)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error querying TIC: {e}\")\n",
    "    print(\"Using fallback: selecting from generated TIC IDs...\")\n",
    "    control_stars = [f\"TIC {i}\" for i in range(100000000, 100000600)]\n",
    "    random.seed(42)\n",
    "    random.shuffle(control_stars)\n",
    "    print(f\"âœ“ {len(control_stars)} fallback control stars available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871abd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightkurve as lk\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "CACHE_DIR = \"lc_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def load_lightcurve(target):\n",
    "    path = os.path.join(CACHE_DIR, target.replace(\" \", \"_\").replace(\"-\", \"_\") + \".fits\")\n",
    "    \n",
    "\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            print(f\"Loading {target} from cache...\")\n",
    "            lc = lk.read(path)\n",
    "            return lc.remove_nans().remove_outliers().normalize()\n",
    "        except Exception as e:\n",
    "            print(f\"Cache read failed for {target}: {e}. Re-downloading...\")\n",
    "            os.remove(path)  \n",
    "    \n",
    "\n",
    "    print(f\"Searching for {target} in TESS archive...\")\n",
    "    \n",
    "\n",
    "    search_results = None\n",
    "    try:\n",
    "        search_results = lk.search_lightcurve(target, mission='TESS', author='SPOC')\n",
    "        if len(search_results) == 0:\n",
    "            print(f\"  No SPOC data, trying all authors...\")\n",
    "            search_results = lk.search_lightcurve(target, mission='TESS')\n",
    "    except Exception as e:\n",
    "        print(f\"  Search error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if search_results is None or len(search_results) == 0:\n",
    "        print(f\"  âœ— No data found for {target}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Found {len(search_results)} results\")\n",
    "    \n",
    "\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            print(f\"  Downloading (attempt {attempt + 1}/3)...\")\n",
    "            lc = search_results[0].download()\n",
    "            \n",
    "            lc.to_fits(path, overwrite=True)\n",
    "            print(f\"  âœ“ Successfully downloaded {target}\")\n",
    "            \n",
    "            return lc.remove_nans().remove_outliers().normalize()\n",
    "        except Exception as e:\n",
    "            print(f\"  Download attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt == 2:\n",
    "                print(f\"  âœ— All download attempts failed for {target}\")\n",
    "                return None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c59534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wotan import flatten\n",
    "import numpy as np\n",
    "\n",
    "def detrend(lc):\n",
    "    try:\n",
    "        time = lc.time.value\n",
    "        flux = lc.flux.value\n",
    "        mask = np.isfinite(time) & np.isfinite(flux)\n",
    "        time = time[mask]\n",
    "        flux = flux[mask]\n",
    "        \n",
    "        if len(time) < 10:\n",
    "            raise ValueError(f\"Insufficient data points: {len(time)}\")\n",
    "        \n",
    "        flat_flux, _ = flatten(time, flux, method='biweight', window_length=0.5, return_trend=True)\n",
    "        return time, flat_flux\n",
    "    except Exception as e:\n",
    "        print(f\"  Detrending error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307799b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transitleastsquares import transitleastsquares\n",
    "from scipy.stats import median_abs_deviation\n",
    "\n",
    "def detect_tls(time, flux):\n",
    "    try:\n",
    "        model = transitleastsquares(time, flux)\n",
    "        results = model.power(use_gpu=True)\n",
    "        \n",
    "        if not hasattr(results, 'period') or results.period is None:\n",
    "            raise ValueError(\"TLS returned invalid results\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"  TLS detection error: {e}\")\n",
    "        raise\n",
    "\n",
    "def calculate_shape_features(time, flux, period, duration, t0):\n",
    "    phase = ((time - t0) % period) / period\n",
    "    phase[phase > 0.5] -= 1.0  # Center around 0\n",
    "    \n",
    "    # Get in-transit points\n",
    "    in_transit = np.abs(phase) < (duration / period / 2)\n",
    "    \n",
    "    if np.sum(in_transit) < 5:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    transit_flux = flux[in_transit]\n",
    "    transit_phase = phase[in_transit]\n",
    "    \n",
    "    # 1. Transit shape symmetry (compare first half vs second half)\n",
    "    mid_idx = len(transit_flux) // 2\n",
    "    first_half = transit_flux[:mid_idx]\n",
    "    second_half = transit_flux[mid_idx:]\n",
    "    symmetry = np.std(first_half - second_half[::-1][:len(first_half)]) if len(first_half) > 0 else 0\n",
    "    \n",
    "    # 2. Ingress/egress duration ratio (V-shape detection)\n",
    "    sorted_indices = np.argsort(transit_flux)\n",
    "    deepest_point = np.median(sorted_indices[:max(1, len(sorted_indices)//5)])\n",
    "    ingress_points = np.sum(transit_phase < 0)\n",
    "    egress_points = np.sum(transit_phase > 0)\n",
    "    shape_ratio = abs(ingress_points - egress_points) / max(ingress_points + egress_points, 1)\n",
    "    \n",
    "    # 3. Transit depth variability\n",
    "    depth_std = np.std(transit_flux)\n",
    "    \n",
    "    return symmetry, shape_ratio, depth_std\n",
    "\n",
    "def odd_even_test(time, flux, period, duration, t0):\n",
    "    phase = ((time - t0) % period) / period\n",
    "    phase[phase > 0.5] -= 1.0\n",
    "    \n",
    "    # Identify transits\n",
    "    in_transit = np.abs(phase) < (duration / period / 2)\n",
    "    transit_number = np.floor((time - t0) / period)\n",
    "    \n",
    "    # Separate odd and even transits\n",
    "    odd_mask = in_transit & (transit_number % 2 == 1)\n",
    "    even_mask = in_transit & (transit_number % 2 == 0)\n",
    "    \n",
    "    if np.sum(odd_mask) < 3 or np.sum(even_mask) < 3:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    odd_flux = flux[odd_mask]\n",
    "    even_flux = flux[even_mask]\n",
    "    \n",
    "    # 1. Depth difference\n",
    "    odd_depth = 1 - np.median(odd_flux)\n",
    "    even_depth = 1 - np.median(even_flux)\n",
    "    depth_diff = abs(odd_depth - even_depth)\n",
    "    \n",
    "    # 2. Duration difference (check if one set is systematically longer)\n",
    "    odd_duration = np.sum(odd_mask) / len(time) * period\n",
    "    even_duration = np.sum(even_mask) / len(time) * period\n",
    "    duration_diff = abs(odd_duration - even_duration) / max(duration, 1e-10)  # Avoid division by zero\n",
    "    \n",
    "    # 3. Shape consistency (MAD of transit depths) - Fixed to avoid inf\n",
    "    mad_odd = median_abs_deviation(odd_flux)\n",
    "    mad_even = median_abs_deviation(even_flux)\n",
    "    \n",
    "    # Safely compute MAD ratio with bounds to prevent inf\n",
    "    if mad_even > 1e-10 and mad_odd > 1e-10:\n",
    "        mad_ratio = mad_odd / mad_even\n",
    "        # Cap the ratio to prevent extreme values\n",
    "        mad_ratio = np.clip(mad_ratio, 0.01, 100)\n",
    "        mad_ratio = abs(np.log(mad_ratio))\n",
    "    else:\n",
    "        mad_ratio = 0\n",
    "    \n",
    "    return depth_diff, duration_diff, mad_ratio\n",
    "\n",
    "def check_multi_sector(target):\n",
    "    try:\n",
    "        search_results = lk.search_lightcurve(target, mission='TESS')\n",
    "        if len(search_results) > 0:\n",
    "            # Count unique sectors\n",
    "            sectors = set()\n",
    "            for i in range(len(search_results)):\n",
    "                if hasattr(search_results[i], 'mission') and hasattr(search_results[i], 'sequence_number'):\n",
    "                    sectors.add(search_results[i].sequence_number)\n",
    "            return len(sectors)\n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bac7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"âœ“ CUDA is available. Using GPU for processing.\")\n",
    "else:\n",
    "    print(\"âœ— CUDA not available. Using CPU for processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install cuml-cu11 --extra-index-url=https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd14431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(target):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing: {target}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    def to_scalar(value):\n",
    "        if value is None:\n",
    "            return 0.0\n",
    "        if isinstance(value, (list, np.ndarray)):\n",
    "            val = float(value[0]) if len(value) > 0 else 0.0\n",
    "        else:\n",
    "            val = float(value)\n",
    "        \n",
    "        if not np.isfinite(val):\n",
    "            return 0.0\n",
    "        return val\n",
    "    \n",
    "    try:\n",
    "        lc = load_lightcurve(target)\n",
    "        if lc is None:\n",
    "            print(f\"âœ— Failed to load lightcurve for {target}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  âœ“ Lightcurve loaded: {len(lc)} data points\")\n",
    "        \n",
    "        # Step 1: Multi-sector check\n",
    "        num_sectors = check_multi_sector(target)\n",
    "        print(f\"  âœ“ Available sectors: {num_sectors}\")\n",
    "        \n",
    "        # Step 2: Detrend\n",
    "        time, flux = detrend(lc)\n",
    "        print(f\"  âœ“ Detrended: {len(time)} clean data points\")\n",
    "        \n",
    "        # Step 3: TLS detection (get full results object)\n",
    "        results = detect_tls(time, flux)\n",
    "        period = to_scalar(results.period)\n",
    "        duration = to_scalar(results.duration)\n",
    "        depth = to_scalar(results.depth)\n",
    "        snr = to_scalar(results.SDE)\n",
    "        t0 = to_scalar(results.T0)\n",
    "        \n",
    "        print(f\"  âœ“ TLS complete: Period={period:.2f}d, Depth={depth:.4f}, SNR={snr:.2f}\")\n",
    "        \n",
    "        # Step 4: SDE threshold check\n",
    "        sde_pass = 1 if snr >= 7.0 else 0  # Standard threshold\n",
    "        print(f\"  âœ“ SDE threshold (7.0): {'PASS' if sde_pass else 'FAIL'} (SNR={snr:.2f})\")\n",
    "        \n",
    "        # Step 5: TLS model fit metrics\n",
    "        rp_rs = to_scalar(results.rp_rs if hasattr(results, 'rp_rs') else 0)\n",
    "        snr_pink = to_scalar(results.snr_pink_per_transit if hasattr(results, 'snr_pink_per_transit') else snr)\n",
    "        odd_even_mismatch = to_scalar(results.odd_even_mismatch if hasattr(results, 'odd_even_mismatch') else 0)\n",
    "        \n",
    "        print(f\"  âœ“ Model fit: Rp/Rs={rp_rs:.4f}, SNR_pink={snr_pink:.2f}\")\n",
    "        \n",
    "        # Step 6: Shape features\n",
    "        symmetry, shape_ratio, depth_std = calculate_shape_features(time, flux, period, duration, t0)\n",
    "        symmetry = to_scalar(symmetry)\n",
    "        shape_ratio = to_scalar(shape_ratio)\n",
    "        depth_std = to_scalar(depth_std)\n",
    "        print(f\"  âœ“ Shape features: symmetry={symmetry:.6f}, ratio={shape_ratio:.4f}\")\n",
    "        \n",
    "        # Step 7: Enhanced odd-even test\n",
    "        depth_diff, duration_diff, mad_ratio = odd_even_test(time, flux, period, duration, t0)\n",
    "        depth_diff = to_scalar(depth_diff)\n",
    "        duration_diff = to_scalar(duration_diff)\n",
    "        mad_ratio = to_scalar(mad_ratio)\n",
    "        print(f\"  âœ“ Odd-even test: depth_diff={depth_diff:.6f}, dur_diff={duration_diff:.4f}\")\n",
    "        \n",
    "\n",
    "        features = [\n",
    "            period,           # 0: Orbital period\n",
    "            depth,            # 1: Transit depth\n",
    "            duration,         # 2: Transit duration\n",
    "            snr,              # 3: Signal-to-noise ratio\n",
    "            sde_pass,         # 4: SDE threshold pass/fail\n",
    "            rp_rs,            # 5: Planet-to-star radius ratio\n",
    "            snr_pink,         # 6: SNR accounting for red noise\n",
    "            odd_even_mismatch,# 7: TLS odd-even mismatch\n",
    "            symmetry,         # 8: Transit symmetry\n",
    "            shape_ratio,      # 9: Ingress/egress ratio\n",
    "            depth_std,        # 10: Transit depth variability\n",
    "            depth_diff,       # 11: Odd-even depth difference\n",
    "            duration_diff,    # 12: Odd-even duration difference\n",
    "            mad_ratio,        # 13: Odd-even MAD ratio\n",
    "            num_sectors,      # 14: Number of sectors\n",
    "            len(time)         # 15: Total data points\n",
    "        ]\n",
    "        \n",
    "\n",
    "        for i, feat in enumerate(features):\n",
    "            if not np.isfinite(feat):\n",
    "                print(f\"  âš  Warning: Feature {i} is not finite ({feat}), setting to 0.0\")\n",
    "                features[i] = 0.0\n",
    "        \n",
    "        print(f\"  âœ“ Extracted {len(features)} features successfully\")\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Feature extraction failed: {type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20af231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "planet_sample = random.sample(planet_targets, min(250, len(planet_targets)))\n",
    "\n",
    "X, y = [], []\n",
    "failed_targets = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"COLLECTING PLANET CANDIDATES ({len(planet_sample)} targets sampled)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for star in planet_sample:\n",
    "    feat = extract_features(star)\n",
    "    if feat:\n",
    "        X.append(feat)\n",
    "        y.append(1)\n",
    "    else:\n",
    "        failed_targets.append((star, \"planet\"))\n",
    "\n",
    "planet_count = sum(y)\n",
    "print(f\"\\nâœ“ Successfully collected {planet_count} planet samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"COLLECTING NON-PLANET STARS ({len(control_stars)} targets sampled)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "target_control_count = max(150, planet_count)\n",
    "\n",
    "for star in control_stars:\n",
    "    feat = extract_features(star)\n",
    "    if feat:\n",
    "        X.append(feat)\n",
    "        y.append(0)\n",
    "        current_control_count = len(y) - sum(y)\n",
    "        if current_control_count >= target_control_count:\n",
    "            print(f\"\\nâœ“ Reached target of {target_control_count} non-planets, stopping collection\")\n",
    "            break\n",
    "    else:\n",
    "        failed_targets.append((star, \"control\"))\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANING DATA\")\n",
    "print(\"=\"*60)\n",
    "X_clean, y_clean = [], []\n",
    "removed_count = 0\n",
    "\n",
    "for i, (features, label) in enumerate(zip(X, y)):\n",
    "    if all(np.isfinite(f) for f in features):\n",
    "        X_clean.append(features)\n",
    "        y_clean.append(label)\n",
    "    else:\n",
    "        removed_count += 1\n",
    "        print(f\"  Removed sample {i}: contains inf/nan\")\n",
    "\n",
    "X, y = X_clean, y_clean\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLLECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ“ Successfully collected: {len(X)} samples\")\n",
    "print(f\"  - Planets: {sum(y)}\")\n",
    "print(f\"  - Non-planets: {len(y) - sum(y)}\")\n",
    "print(f\"  - Removed (inf/nan): {removed_count}\")\n",
    "print(f\"  - Balance ratio: {max(sum(y), len(y)-sum(y)) / min(sum(y), len(y)-sum(y)):.2f}:1\")\n",
    "\n",
    "if failed_targets:\n",
    "    print(f\"\\nâœ— Failed to process {len(failed_targets)} targets:\")\n",
    "    for target, category in failed_targets[:10]:  # Show first 10\n",
    "        print(f\"  - {target} ({category})\")\n",
    "    if len(failed_targets) > 10:\n",
    "        print(f\"  ... and {len(failed_targets) - 10} more\")\n",
    "\n",
    "\n",
    "if sum(y) < 100:\n",
    "    print(f\"\\nâš  WARNING: Only {sum(y)} planets collected (target: 100+)\")\n",
    "if len(y) - sum(y) < 100:\n",
    "    print(f\"\\nâš  WARNING: Only {len(y) - sum(y)} non-planets collected (target: 100+)\")\n",
    "\n",
    "if len(X) < 200:\n",
    "    print(\"\\nâš  WARNING: Dataset has fewer than 200 samples! Model may not be reliable.\")\n",
    "elif len(X) < 300:\n",
    "    print(f\"\\nâš¡ Dataset size: {len(X)} samples (minimum met, but target is 500+)\")\n",
    "elif len(X) < 500:\n",
    "    print(f\"\\nâš¡ Dataset size: {len(X)} samples (good, but target is 500+)\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ Dataset ready for training! ({len(X)} samples - EXCELLENT!)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c941c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASS BALANCE CHECK\")\n",
    "print(\"=\"*60)\n",
    "class_counts = Counter(y)\n",
    "print(f\"Class distribution: {dict(class_counts)}\")\n",
    "print(f\"  - Non-planets (0): {class_counts[0]} ({class_counts[0]/len(y)*100:.1f}%)\")\n",
    "print(f\"  - Planets (1): {class_counts[1]} ({class_counts[1]/len(y)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "imbalance_ratio = max(class_counts.values()) / min(class_counts.values())\n",
    "print(f\"  - Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if imbalance_ratio > 2:\n",
    "    print(\"  âš  Significant class imbalance detected! Using class weights...\")\n",
    "    use_class_weights = True\n",
    "else:\n",
    "    print(\"  âœ“ Classes are balanced\")\n",
    "    use_class_weights = False\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAIN-VALIDATION-TEST SPLIT (70%-15%-15%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Split the 30% temp into 50/50 for validation and test (15% each of total)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Planets: {sum(y_train)} ({sum(y_train)/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  - Non-planets: {len(y_train) - sum(y_train)} ({(len(y_train)-sum(y_train))/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Planets: {sum(y_val)} ({sum(y_val)/len(y_val)*100:.1f}%)\")\n",
    "print(f\"  - Non-planets: {len(y_val) - sum(y_val)} ({(len(y_val)-sum(y_val))/len(y_val)*100:.1f}%)\")\n",
    "\n",
    "print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Planets: {sum(y_test)} ({sum(y_test)/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  - Non-planets: {len(y_test) - sum(y_test)} ({(len(y_test)-sum(y_test))/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "if use_class_weights:\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    print(f\"\\nClass weights: {class_weight_dict}\")\n",
    "else:\n",
    "    class_weight_dict = None\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200, \n",
    "    random_state=42,\n",
    "    class_weight=class_weight_dict,\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on all three sets\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "val_accuracy = model.score(X_val, y_val)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nâœ“ Model trained successfully!\")\n",
    "print(f\"  Training accuracy:   {train_accuracy:.2%}\")\n",
    "print(f\"  Validation accuracy: {val_accuracy:.2%}\")\n",
    "print(f\"  Test accuracy:       {test_accuracy:.2%}\")\n",
    "\n",
    "# Check for overfitting\n",
    "train_val_gap = train_accuracy - val_accuracy\n",
    "print(f\"\\nGeneralization check:\")\n",
    "print(f\"  Train-Val gap: {train_val_gap*100:.1f}%\")\n",
    "if train_val_gap > 0.15:\n",
    "    print(f\"  âš  Warning: Possible overfitting!\")\n",
    "elif train_val_gap > 0.05:\n",
    "    print(f\"  âš¡ Minor overfitting detected\")\n",
    "else:\n",
    "    print(f\"  âœ“ Good generalization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names and importance analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_names = [\n",
    "    'Period',           # 0: Orbital period (days)\n",
    "    'Depth',            # 1: Transit depth\n",
    "    'Duration',         # 2: Transit duration (days)\n",
    "    'SNR',              # 3: Signal-to-noise ratio (SDE)\n",
    "    'SDE_Pass',         # 4: SDE threshold pass (1) or fail (0)\n",
    "    'Rp_Rs',            # 5: Planet-to-star radius ratio\n",
    "    'SNR_Pink',         # 6: SNR accounting for pink noise\n",
    "    'OE_Mismatch',      # 7: TLS odd-even mismatch\n",
    "    'Symmetry',         # 8: Transit symmetry\n",
    "    'Shape_Ratio',      # 9: Ingress/egress ratio\n",
    "    'Depth_Std',        # 10: Depth standard deviation\n",
    "    'OE_Depth_Diff',    # 11: Odd-even depth difference\n",
    "    'OE_Duration_Diff', # 12: Odd-even duration difference\n",
    "    'OE_MAD_Ratio',     # 13: Odd-even MAD ratio\n",
    "    'N_Sectors',        # 14: Number of TESS sectors\n",
    "    'N_Points'          # 15: Total data points\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(\"-\" * 60)\n",
    "for idx, row in feature_importance_df.head(10).iterrows():\n",
    "    bar = 'â–ˆ' * int(row['importance'] * 100)\n",
    "    print(f\"{row['feature']:20s} â”‚ {bar} {row['importance']:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['feature'][:10], feature_importance_df['importance'][:10])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Top 10 Feature Importances - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nâœ“ Feature importance plot saved to: feature_importance.png\")\n",
    "plt.show()\n",
    "\n",
    "# Check if model is learning astrophysics or artifacts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE QUALITY CHECK\")\n",
    "print(\"=\"*60)\n",
    "top_features = feature_importance_df.head(3)['feature'].tolist()\n",
    "physics_features = ['SNR', 'Depth', 'Period', 'Duration', 'SNR_Pink', 'Rp_Rs', 'SDE_Pass']\n",
    "metadata_features = ['N_Sectors', 'N_Points']\n",
    "\n",
    "physics_in_top = sum(1 for f in top_features if f in physics_features)\n",
    "metadata_in_top = sum(1 for f in top_features if f in metadata_features)\n",
    "\n",
    "if metadata_in_top >= 2:\n",
    "    print(\"âš  WARNING: Model may be learning observational bias!\")\n",
    "    print(f\"  Top features include: {', '.join(top_features)}\")\n",
    "    print(\"  Consider removing N_Sectors and N_Points from features.\")\n",
    "elif physics_in_top >= 2:\n",
    "    print(\"âœ“ Model is learning physical features (good!)\")\n",
    "    print(f\"  Top features: {', '.join(top_features)}\")\n",
    "else:\n",
    "    print(\"âš¡ Model is learning shape/consistency features\")\n",
    "    print(f\"  Top features: {', '.join(top_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17be8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced evaluation metrics and cross-validation\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            roc_auc_score, roc_curve, RocCurveDisplay,\n",
    "                            precision_recall_curve, average_precision_score)\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 5-fold stratified cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "cv_f1_scores = cross_val_score(model, X, y, cv=cv, scoring='f1', n_jobs=-1)\n",
    "\n",
    "print(f\"5-Fold Cross-Validation Results:\")\n",
    "print(f\"  Accuracy: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}\")\n",
    "print(f\"  F1 Score: {cv_f1_scores.mean():.3f} Â± {cv_f1_scores.std():.3f}\")\n",
    "print(f\"  Individual folds (accuracy): {[f'{s:.3f}' for s in cv_scores]}\")\n",
    "\n",
    "# Check CV stability\n",
    "if cv_scores.std() < 0.03:\n",
    "    print(f\"  âœ“ STABLE: CV std ({cv_scores.std():.3f}) < 0.03\")\n",
    "else:\n",
    "    print(f\"  âš  UNSTABLE: CV std ({cv_scores.std():.3f}) >= 0.03\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Confusion Matrix for validation\n",
    "cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "tn_val, fp_val, fn_val, tp_val = cm_val.ravel()\n",
    "recall_val = tp_val / (tp_val + fn_val) if (tp_val + fn_val) > 0 else 0\n",
    "specificity_val = tn_val / (tn_val + fp_val) if (tn_val + fp_val) > 0 else 0\n",
    "\n",
    "print(f\"Validation Set Metrics:\")\n",
    "print(f\"  Accuracy:     {val_accuracy:.3f}\")\n",
    "print(f\"  Recall:       {recall_val:.3f} {'âœ“ PASS' if recall_val > 0.7 else 'âœ— FAIL'} (target: > 0.70)\")\n",
    "print(f\"  Specificity:  {specificity_val:.3f} {'âœ“ PASS' if specificity_val > 0.8 else 'âœ— FAIL'} (target: > 0.80)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Non-Planet', 'Planet'], digits=3))\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"                 Non-Planet  Planet\")\n",
    "print(f\"Actual Non-Planet    {cm[0,0]:3d}        {cm[0,1]:3d}\")\n",
    "print(f\"Actual Planet        {cm[1,0]:3d}        {cm[1,1]:3d}\")\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"\\nDetailed Metrics:\")\n",
    "print(f\"  True Positives:  {tp} (correctly identified planets)\")\n",
    "print(f\"  True Negatives:  {tn} (correctly identified non-planets)\")\n",
    "print(f\"  False Positives: {fp} (false alarms)\")\n",
    "print(f\"  False Negatives: {fn} (missed planets)\")\n",
    "print(f\"  Recall (Sensitivity): {sensitivity:.3f} {'âœ“ PASS' if sensitivity > 0.7 else 'âœ— FAIL'} (target: > 0.70)\")\n",
    "print(f\"  Specificity:          {specificity:.3f} {'âœ“ PASS' if specificity > 0.8 else 'âœ— FAIL'} (target: > 0.80)\")\n",
    "\n",
    "# ROC AUC Score\n",
    "try:\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"  ROC AUC Score: {roc_auc:.3f}\")\n",
    "except:\n",
    "    print(\"  ROC AUC Score: N/A (insufficient data)\")\n",
    "    roc_auc = None\n",
    "\n",
    "# Average Precision (better for imbalanced datasets)\n",
    "try:\n",
    "    avg_precision = average_precision_score(y_test, y_proba)\n",
    "    print(f\"  Average Precision: {avg_precision:.3f}\")\n",
    "except:\n",
    "    print(\"  Average Precision: N/A\")\n",
    "    avg_precision = None\n",
    "\n",
    "# Plot ROC and Precision-Recall curves\n",
    "if roc_auc is not None and len(np.unique(y_test)) > 1:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ROC Curve\n",
    "    RocCurveDisplay.from_estimator(model, X_test, y_test, ax=ax1)\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "    ax1.set_title(f'ROC Curve (AUC = {roc_auc:.3f})', fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    from sklearn.metrics import PrecisionRecallDisplay\n",
    "    PrecisionRecallDisplay.from_estimator(model, X_test, y_test, ax=ax2)\n",
    "    ax2.set_title(f'Precision-Recall Curve (AP = {avg_precision:.3f})', fontweight='bold')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('evaluation_curves.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Evaluation curves saved to: evaluation_curves.png\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ“ Random Forest with {model.n_estimators} trees\")\n",
    "print(f\"âœ“ Trained on {len(X_train)} samples\")\n",
    "print(f\"âœ“ Validated on {len(X_val)} samples\")\n",
    "print(f\"âœ“ Tested on {len(X_test)} samples\")\n",
    "print(f\"\\nAccuracy:\")\n",
    "print(f\"  Train: {train_accuracy:.2%}\")\n",
    "print(f\"  Val:   {val_accuracy:.2%}\")\n",
    "print(f\"  Test:  {test_accuracy:.2%}\")\n",
    "print(f\"\\nCross-Validation:\")\n",
    "print(f\"  Mean: {cv_scores.mean():.2%} Â± {cv_scores.std():.2%}\")\n",
    "if roc_auc:\n",
    "    print(f\"\\nROC AUC: {roc_auc:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REQUIREMENTS CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset Size:    {len(X)} samples {'âœ“' if len(X) >= 200 else 'âœ—'} (minimum: 200)\")\n",
    "print(f\"  Planets:       {sum(y)} {'âœ“' if sum(y) >= 100 else 'âœ—'} (minimum: 100)\")\n",
    "print(f\"  Non-planets:   {len(y)-sum(y)} {'âœ“' if len(y)-sum(y) >= 100 else 'âœ—'} (minimum: 100)\")\n",
    "print(f\"Recall (Test):   {sensitivity:.3f} {'âœ“ PASS' if sensitivity > 0.7 else 'âœ— FAIL'} (target: > 0.70)\")\n",
    "print(f\"Specificity:     {specificity:.3f} {'âœ“ PASS' if specificity > 0.8 else 'âœ— FAIL'} (target: > 0.80)\")\n",
    "print(f\"CV Stability:    {cv_scores.std():.3f} {'âœ“ PASS' if cv_scores.std() < 0.03 else 'âœ— FAIL'} (target: < 0.03)\")\n",
    "\n",
    "all_pass = (len(X) >= 200 and sum(y) >= 100 and (len(y)-sum(y)) >= 100 and \n",
    "            sensitivity > 0.7 and specificity > 0.8 and cv_scores.std() < 0.03)\n",
    "\n",
    "if all_pass:\n",
    "    print(\"\\nðŸŽ‰ ALL REQUIREMENTS MET! Model is ready for production.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Some requirements not met. Consider:\")\n",
    "    if len(X) < 200 or sum(y) < 100 or (len(y)-sum(y)) < 100:\n",
    "        print(\"  - Collecting more data samples\")\n",
    "    if sensitivity <= 0.7:\n",
    "        print(\"  - Adjust class weights to improve recall\")\n",
    "    if specificity <= 0.8:\n",
    "        print(\"  - Review false positive cases\")\n",
    "    if cv_scores.std() >= 0.03:\n",
    "        print(\"  - Increase dataset size for more stable CV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e117da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERSISTENCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "model_package = {\n",
    "    'model': model,\n",
    "    'feature_names': feature_names,\n",
    "    'training_metadata': {\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'n_train_samples': len(X_train),\n",
    "        'n_val_samples': len(X_val),\n",
    "        'n_test_samples': len(X_test),\n",
    "        'n_features': len(feature_names),\n",
    "        'train_accuracy': float(train_accuracy),\n",
    "        'val_accuracy': float(val_accuracy),\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'cv_accuracy_mean': float(cv_scores.mean()),\n",
    "        'cv_accuracy_std': float(cv_scores.std()),\n",
    "        'test_recall': float(sensitivity),\n",
    "        'test_specificity': float(specificity),\n",
    "        'class_distribution': dict(Counter(y)),\n",
    "        'hyperparameters': model.get_params(),\n",
    "        'used_class_weights': use_class_weights\n",
    "    },\n",
    "    'feature_importances': dict(zip(feature_names, model.feature_importances_))\n",
    "}\n",
    "\n",
    "\n",
    "model_filename = 'exoplanet_model_v2.pkl'\n",
    "joblib.dump(model_package, model_filename)\n",
    "\n",
    "print(f\"âœ“ Model saved to: {model_filename}\")\n",
    "print(f\"  Model type: Random Forest\")\n",
    "print(f\"  Number of trees: {model.n_estimators}\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Validation samples: {len(X_val)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "print(f\"  Test accuracy: {test_accuracy:.2%}\")\n",
    "print(f\"  Test recall: {sensitivity:.2%}\")\n",
    "print(f\"  Test specificity: {specificity:.2%}\")\n",
    "print(f\"  File size: {os.path.getsize(model_filename) / 1024:.1f} KB\")\n",
    "\n",
    "print(\"\\nTo load the model later:\")\n",
    "print(\"  loaded = joblib.load('exoplanet_model_v2.pkl')\")\n",
    "print(\"  model = loaded['model']\")\n",
    "print(\"  feature_names = loaded['feature_names']\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ PIPELINE UPDATED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nImplemented improvements:\")\n",
    "print(\"  âœ“ 70%-15%-15% train-validation-test split\")\n",
    "print(\"  âœ“ Balanced classes (1:1 ratio target)\")\n",
    "print(\"  âœ“ Minimum 200 samples (100 planets + 100 non-planets)\")\n",
    "print(\"  âœ“ Performance targets: Recall > 0.7, Specificity > 0.8\")\n",
    "print(\"  âœ“ CV stability check: std < 0.03\")\n",
    "print(\"  âœ“ Comprehensive validation set evaluation\")\n",
    "print(\"\\nModel is ready for production use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4b7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_star(target, save_to_csv=True):\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PREDICTION FOR: {target}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    features = extract_features(target)\n",
    "    \n",
    "    if features is None:\n",
    "        print(f\"\\nâœ— Could not make prediction for {target} (feature extraction failed)\")\n",
    "        return None\n",
    "    \n",
    "    features_array = np.array(features).reshape(1, -1)\n",
    "    prediction = model.predict(features_array)[0]\n",
    "    probability = model.predict_proba(features_array)[0][1]  # Probability of being a planet\n",
    "    \n",
    "    result = \"Planet\" if prediction == 1 else \"Non-Planet\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PREDICTION RESULT: {result}\")\n",
    "    print(f\"Planet Probability: {probability:.2%}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if save_to_csv:\n",
    "        import pandas as pd\n",
    "        from datetime import datetime\n",
    "        \n",
    "        csv_file = 'predictions.csv'\n",
    "        \n",
    "        record = {\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'target_name': target,\n",
    "            'prediction': result,\n",
    "            'planet_probability': probability\n",
    "        }\n",
    "        \n",
    "        for i, name in enumerate(feature_names):\n",
    "            record[name.lower().replace(' ', '_')] = features[i]\n",
    "        \n",
    "        df_new = pd.DataFrame([record])\n",
    "        \n",
    "        try:\n",
    "            if Path(csv_file).exists():\n",
    "                df_existing = pd.read_csv(csv_file)\n",
    "                df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "            else:\n",
    "                df_combined = df_new\n",
    "            \n",
    "            df_combined.to_csv(csv_file, index=False)\n",
    "            print(f\"âœ“ Prediction saved to {csv_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save to CSV: {e}\")\n",
    "    \n",
    "    return {'target': target, 'prediction': result, 'probability': probability, 'features': features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf458fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing the model on new targets:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "predict_star(\"TOI-270\")\n",
    "\n",
    "print()\n",
    "\n",
    "predict_star(\"TIC 38846515\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae048b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = extract_features(\"TIC 307210830\")\n",
    "if test_features:\n",
    "    print(f\"\\n\\nExtracted {len(test_features)} features:\")\n",
    "    print(f\"  Period: {test_features[0]:.2f} days\")\n",
    "    print(f\"  Depth: {test_features[1]:.6f}\")\n",
    "    print(f\"  Duration: {test_features[2]:.4f} days\")\n",
    "    print(f\"  SNR: {test_features[3]:.2f}\")\n",
    "    print(f\"  SDE Pass: {test_features[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e911c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets = [\"TIC 16740101\",\"TIC 188350478\",\"TIC 38964113\",\"TIC 377909730\"]\n",
    "\n",
    "print(\"Running batch predictions...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for target in test_targets:\n",
    "    predict_star(target)\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTIONS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nâœ“ All predictions saved to: predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_star(\"Ross 176\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
