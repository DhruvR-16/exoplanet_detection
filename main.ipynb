{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e055392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616 planet hosts found\n"
     ]
    }
   ],
   "source": [
    "from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\n",
    "\n",
    "planets = NasaExoplanetArchive.query_criteria(\n",
    "    table=\"pscomppars\",\n",
    "    select=\"hostname, discoverymethod, disc_facility\",\n",
    "    where=\"discoverymethod like 'Transit'\"\n",
    ").to_pandas()\n",
    "\n",
    "\n",
    "planet_targets = planets[\n",
    "    planets['disc_facility'].str.contains('TESS', na=False)\n",
    "]['hostname'].unique().tolist()\n",
    "\n",
    "print(len(planet_targets), \"planet hosts found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "560d51e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: InputWarning: Coordinate string is being interpreted as an ICRS coordinate. [astroquery.utils.commons]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error querying TIC: Timeout limit of 600 exceeded.\n",
      "Using fallback: selecting from cached targets...\n",
      "✓ 50 fallback control stars selected\n"
     ]
    }
   ],
   "source": [
    "from astroquery.mast import Catalogs\n",
    "import random\n",
    "\n",
    "try:\n",
    "    tic_query = Catalogs.query_region(\n",
    "        \"00h42m44s +41d16m09s\",  # Andromeda region (well-observed by TESS)\n",
    "        radius=5,  # degrees\n",
    "        catalog=\"TIC\"\n",
    "    )\n",
    "    \n",
    "    mask = (tic_query['Tmag'] > 8) & (tic_query['Tmag'] < 13)\n",
    "    filtered_tic = tic_query[mask]\n",
    "    \n",
    "    all_tic_targets = [f\"TIC {int(tic)}\" for tic in filtered_tic['ID'][:500]]\n",
    "    \n",
    "    control_stars = [t for t in all_tic_targets if t not in planet_targets]\n",
    "    \n",
    "    random.seed(42)\n",
    "    control_stars = random.sample(control_stars, min(200, len(control_stars)))\n",
    "    \n",
    "    print(f\"✓ {len(control_stars)} control stars selected\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error querying TIC: {e}\")\n",
    "    print(\"Using fallback: selecting from cached targets...\")\n",
    "    control_stars = [f\"TIC {i}\" for i in range(100000000, 100000200)]\n",
    "    random.seed(42)\n",
    "    control_stars = random.sample(control_stars, 50)\n",
    "    print(f\"✓ {len(control_stars)} fallback control stars selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871abd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightkurve as lk\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "CACHE_DIR = \"lc_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def load_lightcurve(target):\n",
    "    path = os.path.join(CACHE_DIR, target.replace(\" \", \"_\").replace(\"-\", \"_\") + \".fits\")\n",
    "    \n",
    "\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            print(f\"Loading {target} from cache...\")\n",
    "            lc = lk.read(path)\n",
    "            return lc.remove_nans().remove_outliers().normalize()\n",
    "        except Exception as e:\n",
    "            print(f\"Cache read failed for {target}: {e}. Re-downloading...\")\n",
    "            os.remove(path)  \n",
    "    \n",
    "\n",
    "    print(f\"Searching for {target} in TESS archive...\")\n",
    "    \n",
    "\n",
    "    search_results = None\n",
    "    try:\n",
    "        search_results = lk.search_lightcurve(target, mission='TESS', author='SPOC')\n",
    "        if len(search_results) == 0:\n",
    "            print(f\"  No SPOC data, trying all authors...\")\n",
    "            search_results = lk.search_lightcurve(target, mission='TESS')\n",
    "    except Exception as e:\n",
    "        print(f\"  Search error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if search_results is None or len(search_results) == 0:\n",
    "        print(f\"  ✗ No data found for {target}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Found {len(search_results)} results\")\n",
    "    \n",
    "\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            print(f\"  Downloading (attempt {attempt + 1}/3)...\")\n",
    "            lc = search_results[0].download()\n",
    "            \n",
    "            lc.to_fits(path, overwrite=True)\n",
    "            print(f\"  ✓ Successfully downloaded {target}\")\n",
    "            \n",
    "            return lc.remove_nans().remove_outliers().normalize()\n",
    "        except Exception as e:\n",
    "            print(f\"  Download attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt == 2:\n",
    "                print(f\"  ✗ All download attempts failed for {target}\")\n",
    "                return None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59c59534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wotan import flatten\n",
    "import numpy as np\n",
    "\n",
    "def detrend(lc):\n",
    "    try:\n",
    "        time = lc.time.value\n",
    "        flux = lc.flux.value\n",
    "        mask = np.isfinite(time) & np.isfinite(flux)\n",
    "        time = time[mask]\n",
    "        flux = flux[mask]\n",
    "        \n",
    "        if len(time) < 10:\n",
    "            raise ValueError(f\"Insufficient data points: {len(time)}\")\n",
    "        \n",
    "        flat_flux, _ = flatten(time, flux, method='biweight', window_length=0.5, return_trend=True)\n",
    "        return time, flat_flux\n",
    "    except Exception as e:\n",
    "        print(f\"  Detrending error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307799b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transitleastsquares import transitleastsquares\n",
    "from scipy.stats import median_abs_deviation\n",
    "\n",
    "def detect_tls(time, flux):\n",
    "    try:\n",
    "        model = transitleastsquares(time, flux)\n",
    "        results = model.power()\n",
    "        \n",
    "        if not hasattr(results, 'period') or results.period is None:\n",
    "            raise ValueError(\"TLS returned invalid results\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"  TLS detection error: {e}\")\n",
    "        raise\n",
    "\n",
    "def calculate_shape_features(time, flux, period, duration, t0):\n",
    "    phase = ((time - t0) % period) / period\n",
    "    phase[phase > 0.5] -= 1.0  # Center around 0\n",
    "    \n",
    "    # Get in-transit points\n",
    "    in_transit = np.abs(phase) < (duration / period / 2)\n",
    "    \n",
    "    if np.sum(in_transit) < 5:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    transit_flux = flux[in_transit]\n",
    "    transit_phase = phase[in_transit]\n",
    "    \n",
    "    # 1. Transit shape symmetry (compare first half vs second half)\n",
    "    mid_idx = len(transit_flux) // 2\n",
    "    first_half = transit_flux[:mid_idx]\n",
    "    second_half = transit_flux[mid_idx:]\n",
    "    symmetry = np.std(first_half - second_half[::-1][:len(first_half)]) if len(first_half) > 0 else 0\n",
    "    \n",
    "    # 2. Ingress/egress duration ratio (V-shape detection)\n",
    "    sorted_indices = np.argsort(transit_flux)\n",
    "    deepest_point = np.median(sorted_indices[:max(1, len(sorted_indices)//5)])\n",
    "    ingress_points = np.sum(transit_phase < 0)\n",
    "    egress_points = np.sum(transit_phase > 0)\n",
    "    shape_ratio = abs(ingress_points - egress_points) / max(ingress_points + egress_points, 1)\n",
    "    \n",
    "    # 3. Transit depth variability\n",
    "    depth_std = np.std(transit_flux)\n",
    "    \n",
    "    return symmetry, shape_ratio, depth_std\n",
    "\n",
    "def odd_even_test_enhanced(time, flux, period, duration, t0):\n",
    "    phase = ((time - t0) % period) / period\n",
    "    phase[phase > 0.5] -= 1.0\n",
    "    \n",
    "    # Identify transits\n",
    "    in_transit = np.abs(phase) < (duration / period / 2)\n",
    "    transit_number = np.floor((time - t0) / period)\n",
    "    \n",
    "    # Separate odd and even transits\n",
    "    odd_mask = in_transit & (transit_number % 2 == 1)\n",
    "    even_mask = in_transit & (transit_number % 2 == 0)\n",
    "    \n",
    "    if np.sum(odd_mask) < 3 or np.sum(even_mask) < 3:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    odd_flux = flux[odd_mask]\n",
    "    even_flux = flux[even_mask]\n",
    "    \n",
    "    # 1. Depth difference\n",
    "    odd_depth = 1 - np.median(odd_flux)\n",
    "    even_depth = 1 - np.median(even_flux)\n",
    "    depth_diff = abs(odd_depth - even_depth)\n",
    "    \n",
    "    # 2. Duration difference (check if one set is systematically longer)\n",
    "    odd_duration = np.sum(odd_mask) / len(time) * period\n",
    "    even_duration = np.sum(even_mask) / len(time) * period\n",
    "    duration_diff = abs(odd_duration - even_duration) / max(duration, 1e-10)  # Avoid division by zero\n",
    "    \n",
    "    # 3. Shape consistency (MAD of transit depths) - Fixed to avoid inf\n",
    "    mad_odd = median_abs_deviation(odd_flux)\n",
    "    mad_even = median_abs_deviation(even_flux)\n",
    "    \n",
    "    # Safely compute MAD ratio with bounds to prevent inf\n",
    "    if mad_even > 1e-10 and mad_odd > 1e-10:\n",
    "        mad_ratio = mad_odd / mad_even\n",
    "        # Cap the ratio to prevent extreme values\n",
    "        mad_ratio = np.clip(mad_ratio, 0.01, 100)\n",
    "        mad_ratio = abs(np.log(mad_ratio))\n",
    "    else:\n",
    "        mad_ratio = 0\n",
    "    \n",
    "    return depth_diff, duration_diff, mad_ratio\n",
    "\n",
    "def check_multi_sector(target):\n",
    "    try:\n",
    "        search_results = lk.search_lightcurve(target, mission='TESS')\n",
    "        if len(search_results) > 0:\n",
    "            # Count unique sectors\n",
    "            sectors = set()\n",
    "            for i in range(len(search_results)):\n",
    "                if hasattr(search_results[i], 'mission') and hasattr(search_results[i], 'sequence_number'):\n",
    "                    sectors.add(search_results[i].sequence_number)\n",
    "            return len(sectors)\n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd14431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(target):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing: {target}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    def to_scalar(value):\n",
    "        if value is None:\n",
    "            return 0.0\n",
    "        if isinstance(value, (list, np.ndarray)):\n",
    "            val = float(value[0]) if len(value) > 0 else 0.0\n",
    "        else:\n",
    "            val = float(value)\n",
    "        \n",
    "        if not np.isfinite(val):\n",
    "            return 0.0\n",
    "        return val\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load lightcurve\n",
    "        lc = load_lightcurve(target)\n",
    "        if lc is None:\n",
    "            print(f\"✗ Failed to load lightcurve for {target}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  ✓ Lightcurve loaded: {len(lc)} data points\")\n",
    "        \n",
    "        # Step 2: Multi-sector check\n",
    "        num_sectors = check_multi_sector(target)\n",
    "        print(f\"  ✓ Available sectors: {num_sectors}\")\n",
    "        \n",
    "        # Step 3: Detrend\n",
    "        time, flux = detrend(lc)\n",
    "        print(f\"  ✓ Detrended: {len(time)} clean data points\")\n",
    "        \n",
    "        # Step 4: TLS detection (get full results object)\n",
    "        results = detect_tls(time, flux)\n",
    "        period = to_scalar(results.period)\n",
    "        duration = to_scalar(results.duration)\n",
    "        depth = to_scalar(results.depth)\n",
    "        snr = to_scalar(results.SDE)\n",
    "        t0 = to_scalar(results.T0)\n",
    "        \n",
    "        print(f\"  ✓ TLS complete: Period={period:.2f}d, Depth={depth:.4f}, SNR={snr:.2f}\")\n",
    "        \n",
    "        # Step 5: SDE threshold check\n",
    "        sde_pass = 1 if snr >= 7.0 else 0  # Standard threshold\n",
    "        print(f\"  ✓ SDE threshold (7.0): {'PASS' if sde_pass else 'FAIL'} (SNR={snr:.2f})\")\n",
    "        \n",
    "        # Step 6: TLS model fit metrics\n",
    "        rp_rs = to_scalar(results.rp_rs if hasattr(results, 'rp_rs') else 0)\n",
    "        snr_pink = to_scalar(results.snr_pink_per_transit if hasattr(results, 'snr_pink_per_transit') else snr)\n",
    "        odd_even_mismatch = to_scalar(results.odd_even_mismatch if hasattr(results, 'odd_even_mismatch') else 0)\n",
    "        \n",
    "        print(f\"  ✓ Model fit: Rp/Rs={rp_rs:.4f}, SNR_pink={snr_pink:.2f}\")\n",
    "        \n",
    "        # Step 7: Shape features\n",
    "        symmetry, shape_ratio, depth_std = calculate_shape_features(time, flux, period, duration, t0)\n",
    "        symmetry = to_scalar(symmetry)\n",
    "        shape_ratio = to_scalar(shape_ratio)\n",
    "        depth_std = to_scalar(depth_std)\n",
    "        print(f\"  ✓ Shape features: symmetry={symmetry:.6f}, ratio={shape_ratio:.4f}\")\n",
    "        \n",
    "        # Step 8: Enhanced odd-even test\n",
    "        depth_diff, duration_diff, mad_ratio = odd_even_test_enhanced(time, flux, period, duration, t0)\n",
    "        depth_diff = to_scalar(depth_diff)\n",
    "        duration_diff = to_scalar(duration_diff)\n",
    "        mad_ratio = to_scalar(mad_ratio)\n",
    "        print(f\"  ✓ Odd-even test: depth_diff={depth_diff:.6f}, dur_diff={duration_diff:.4f}\")\n",
    "        \n",
    "\n",
    "        features = [\n",
    "            period,           # 0: Orbital period\n",
    "            depth,            # 1: Transit depth\n",
    "            duration,         # 2: Transit duration\n",
    "            snr,              # 3: Signal-to-noise ratio\n",
    "            sde_pass,         # 4: SDE threshold pass/fail\n",
    "            rp_rs,            # 5: Planet-to-star radius ratio\n",
    "            snr_pink,         # 6: SNR accounting for red noise\n",
    "            odd_even_mismatch,# 7: TLS odd-even mismatch\n",
    "            symmetry,         # 8: Transit symmetry\n",
    "            shape_ratio,      # 9: Ingress/egress ratio\n",
    "            depth_std,        # 10: Transit depth variability\n",
    "            depth_diff,       # 11: Odd-even depth difference\n",
    "            duration_diff,    # 12: Odd-even duration difference\n",
    "            mad_ratio,        # 13: Odd-even MAD ratio\n",
    "            num_sectors,      # 14: Number of sectors\n",
    "            len(time)         # 15: Total data points\n",
    "        ]\n",
    "        \n",
    "\n",
    "        for i, feat in enumerate(features):\n",
    "            if not np.isfinite(feat):\n",
    "                print(f\"  ⚠ Warning: Feature {i} is not finite ({feat}), setting to 0.0\")\n",
    "                features[i] = 0.0\n",
    "        \n",
    "        print(f\"  ✓ Extracted {len(features)} features successfully\")\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Feature extraction failed: {type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20af231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COLLECTING PLANET CANDIDATES (15 targets)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Analyzing: TOI-6255\n",
      "============================================================\n",
      "Searching for TOI-6255 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TOI-6255\n",
      "\n",
      "============================================================\n",
      "Analyzing: TOI-5728\n",
      "============================================================\n",
      "Loading TOI-5728 from cache...\n",
      "Cache read failed for TOI-5728: The unit 'electron / s' is unrecognized.  It can not be converted to other units.. Re-downloading...\n",
      "Searching for TOI-5728 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TOI-5728\n",
      "\n",
      "============================================================\n",
      "Analyzing: TOI-1452\n",
      "============================================================\n",
      "Searching for TOI-1452 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TOI-1452\n",
      "\n",
      "============================================================\n",
      "Analyzing: TOI-3362\n",
      "============================================================\n",
      "Searching for TOI-3362 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TOI-3362\n",
      "\n",
      "============================================================\n",
      "Analyzing: LTT 9779\n",
      "============================================================\n",
      "Searching for LTT 9779 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for LTT 9779\n",
      "\n",
      "============================================================\n",
      "Analyzing: TOI-880\n",
      "============================================================\n",
      "Searching for TOI-880 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TOI-880\n",
      "\n",
      "============================================================\n",
      "Analyzing: HD 109833\n",
      "============================================================\n",
      "Searching for HD 109833 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for HD 109833\n",
      "\n",
      "============================================================\n",
      "Analyzing: TOI-2407\n",
      "============================================================\n",
      "Searching for TOI-2407 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TOI-2407\n",
      "\n",
      "============================================================\n",
      "Analyzing: WASP-132\n",
      "============================================================\n",
      "Searching for WASP-132 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for WASP-132\n",
      "\n",
      "============================================================\n",
      "Analyzing: TOI-4562\n",
      "============================================================\n",
      "Searching for TOI-4562 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TOI-4562\n",
      "\n",
      "============================================================\n",
      "Analyzing: TOI-1301\n",
      "============================================================\n",
      "Searching for TOI-1301 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TOI-1301\n",
      "\n",
      "============================================================\n",
      "Analyzing: TOI-6109\n",
      "============================================================\n",
      "Loading TOI-6109 from cache...\n",
      "Cache read failed for TOI-6109: The unit 'electron / s' is unrecognized.  It can not be converted to other units.. Re-downloading...\n",
      "Searching for TOI-6109 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TOI-6109\n",
      "\n",
      "============================================================\n",
      "Analyzing: HIP 9618\n",
      "============================================================\n",
      "Loading HIP 9618 from cache...\n",
      "Cache read failed for HIP 9618: The unit 'electron / s' is unrecognized.  It can not be converted to other units.. Re-downloading...\n",
      "Searching for HIP 9618 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for HIP 9618\n",
      "\n",
      "============================================================\n",
      "Analyzing: TOI-1472\n",
      "============================================================\n",
      "Searching for TOI-1472 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TOI-1472\n",
      "\n",
      "============================================================\n",
      "Analyzing: HD 213885\n",
      "============================================================\n",
      "Searching for HD 213885 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for HD 213885\n",
      "\n",
      "============================================================\n",
      "COLLECTING NON-PLANET STARS (50 targets)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000163\n",
      "============================================================\n",
      "Searching for TIC 100000163 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000163\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000028\n",
      "============================================================\n",
      "Searching for TIC 100000028 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000028\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000006\n",
      "============================================================\n",
      "Searching for TIC 100000006 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000006\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000189\n",
      "============================================================\n",
      "Searching for TIC 100000189 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000189\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000070\n",
      "============================================================\n",
      "Searching for TIC 100000070 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000070\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000062\n",
      "============================================================\n",
      "Searching for TIC 100000062 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000062\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000057\n",
      "============================================================\n",
      "Searching for TIC 100000057 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000057\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000035\n",
      "============================================================\n",
      "Searching for TIC 100000035 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000035\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000188\n",
      "============================================================\n",
      "Searching for TIC 100000188 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000188\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000026\n",
      "============================================================\n",
      "Searching for TIC 100000026 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000026\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000173\n",
      "============================================================\n",
      "Searching for TIC 100000173 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000173\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000139\n",
      "============================================================\n",
      "Searching for TIC 100000139 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000139\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000022\n",
      "============================================================\n",
      "Searching for TIC 100000022 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000022\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000151\n",
      "============================================================\n",
      "Searching for TIC 100000151 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000151\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000108\n",
      "============================================================\n",
      "Searching for TIC 100000108 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000108\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000008\n",
      "============================================================\n",
      "Searching for TIC 100000008 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000008\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000007\n",
      "============================================================\n",
      "Searching for TIC 100000007 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000007\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000023\n",
      "============================================================\n",
      "Searching for TIC 100000023 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000023\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000055\n",
      "============================================================\n",
      "Searching for TIC 100000055 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000055\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000059\n",
      "============================================================\n",
      "Searching for TIC 100000059 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000059\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000129\n",
      "============================================================\n",
      "Searching for TIC 100000129 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000129\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000154\n",
      "============================================================\n",
      "Searching for TIC 100000154 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000154\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000197\n",
      "============================================================\n",
      "Searching for TIC 100000197 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000197\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000143\n",
      "============================================================\n",
      "Searching for TIC 100000143 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000143\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000050\n",
      "============================================================\n",
      "Searching for TIC 100000050 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000050\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000166\n",
      "============================================================\n",
      "Searching for TIC 100000166 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000166\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000191\n",
      "============================================================\n",
      "Searching for TIC 100000191 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000191\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000107\n",
      "============================================================\n",
      "Searching for TIC 100000107 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000107\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000056\n",
      "============================================================\n",
      "Searching for TIC 100000056 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000056\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000114\n",
      "============================================================\n",
      "Searching for TIC 100000114 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000114\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000150\n",
      "============================================================\n",
      "Searching for TIC 100000150 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000150\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000071\n",
      "============================================================\n",
      "Searching for TIC 100000071 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000071\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000001\n",
      "============================================================\n",
      "Searching for TIC 100000001 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000001\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000040\n",
      "============================================================\n",
      "Searching for TIC 100000040 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000040\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000185\n",
      "============================================================\n",
      "Searching for TIC 100000185 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000185\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000087\n",
      "============================================================\n",
      "Searching for TIC 100000087 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000087\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000168\n",
      "============================================================\n",
      "Searching for TIC 100000168 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000168\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000039\n",
      "============================================================\n",
      "Searching for TIC 100000039 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000039\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000181\n",
      "============================================================\n",
      "Searching for TIC 100000181 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000181\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000086\n",
      "============================================================\n",
      "Searching for TIC 100000086 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000086\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000190\n",
      "============================================================\n",
      "Searching for TIC 100000190 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000190\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000182\n",
      "============================================================\n",
      "Searching for TIC 100000182 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000182\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000097\n",
      "============================================================\n",
      "Searching for TIC 100000097 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000097\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000024\n",
      "============================================================\n",
      "Searching for TIC 100000024 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000024\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000091\n",
      "============================================================\n",
      "Searching for TIC 100000091 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000091\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000088\n",
      "============================================================\n",
      "Searching for TIC 100000088 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000088\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000067\n",
      "============================================================\n",
      "Searching for TIC 100000067 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000067\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000011\n",
      "============================================================\n",
      "Searching for TIC 100000011 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000011\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000117\n",
      "============================================================\n",
      "Searching for TIC 100000117 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000117\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 100000137\n",
      "============================================================\n",
      "Searching for TIC 100000137 in TESS archive...\n",
      "  Search error: Logon failed for login 'STSCI\\mastiisdist' due to trigger execution.\n",
      "Changed database context to 'CAOMv240'.\n",
      "Changed language setting to us_english.\n",
      "✗ Failed to load lightcurve for TIC 100000137\n",
      "\n",
      "============================================================\n",
      "CLEANING DATA\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "COLLECTION SUMMARY\n",
      "============================================================\n",
      "✓ Successfully collected: 0 samples\n",
      "  - Planets: 0\n",
      "  - Non-planets: 0\n",
      "  - Removed (inf/nan): 0\n",
      "\n",
      "✗ Failed to process 65 targets:\n",
      "  - TOI-6255 (planet)\n",
      "  - TOI-5728 (planet)\n",
      "  - TOI-1452 (planet)\n",
      "  - TOI-3362 (planet)\n",
      "  - LTT 9779 (planet)\n",
      "  - TOI-880 (planet)\n",
      "  - HD 109833 (planet)\n",
      "  - TOI-2407 (planet)\n",
      "  - WASP-132 (planet)\n",
      "  - TOI-4562 (planet)\n",
      "  - TOI-1301 (planet)\n",
      "  - TOI-6109 (planet)\n",
      "  - HIP 9618 (planet)\n",
      "  - TOI-1472 (planet)\n",
      "  - HD 213885 (planet)\n",
      "  - TIC 100000163 (control)\n",
      "  - TIC 100000028 (control)\n",
      "  - TIC 100000006 (control)\n",
      "  - TIC 100000189 (control)\n",
      "  - TIC 100000070 (control)\n",
      "  - TIC 100000062 (control)\n",
      "  - TIC 100000057 (control)\n",
      "  - TIC 100000035 (control)\n",
      "  - TIC 100000188 (control)\n",
      "  - TIC 100000026 (control)\n",
      "  - TIC 100000173 (control)\n",
      "  - TIC 100000139 (control)\n",
      "  - TIC 100000022 (control)\n",
      "  - TIC 100000151 (control)\n",
      "  - TIC 100000108 (control)\n",
      "  - TIC 100000008 (control)\n",
      "  - TIC 100000007 (control)\n",
      "  - TIC 100000023 (control)\n",
      "  - TIC 100000055 (control)\n",
      "  - TIC 100000059 (control)\n",
      "  - TIC 100000129 (control)\n",
      "  - TIC 100000154 (control)\n",
      "  - TIC 100000197 (control)\n",
      "  - TIC 100000143 (control)\n",
      "  - TIC 100000050 (control)\n",
      "  - TIC 100000166 (control)\n",
      "  - TIC 100000191 (control)\n",
      "  - TIC 100000107 (control)\n",
      "  - TIC 100000056 (control)\n",
      "  - TIC 100000114 (control)\n",
      "  - TIC 100000150 (control)\n",
      "  - TIC 100000071 (control)\n",
      "  - TIC 100000001 (control)\n",
      "  - TIC 100000040 (control)\n",
      "  - TIC 100000185 (control)\n",
      "  - TIC 100000087 (control)\n",
      "  - TIC 100000168 (control)\n",
      "  - TIC 100000039 (control)\n",
      "  - TIC 100000181 (control)\n",
      "  - TIC 100000086 (control)\n",
      "  - TIC 100000190 (control)\n",
      "  - TIC 100000182 (control)\n",
      "  - TIC 100000097 (control)\n",
      "  - TIC 100000024 (control)\n",
      "  - TIC 100000091 (control)\n",
      "  - TIC 100000088 (control)\n",
      "  - TIC 100000067 (control)\n",
      "  - TIC 100000011 (control)\n",
      "  - TIC 100000117 (control)\n",
      "  - TIC 100000137 (control)\n",
      "\n",
      "⚠ WARNING: Very small dataset! Model may not be reliable.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "planet_sample = random.sample(planet_targets, min(15, len(planet_targets)))\n",
    "\n",
    "X, y = [], []\n",
    "failed_targets = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"COLLECTING PLANET CANDIDATES ({len(planet_sample)} targets)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for star in planet_sample:\n",
    "    feat = extract_features(star)\n",
    "    if feat:\n",
    "        X.append(feat)\n",
    "        y.append(1)\n",
    "    else:\n",
    "        failed_targets.append((star, \"planet\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"COLLECTING NON-PLANET STARS ({len(control_stars)} targets)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for star in control_stars:\n",
    "    feat = extract_features(star)\n",
    "    if feat:\n",
    "        X.append(feat)\n",
    "        y.append(0)\n",
    "    else:\n",
    "        failed_targets.append((star, \"control\"))\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANING DATA\")\n",
    "print(\"=\"*60)\n",
    "X_clean, y_clean = [], []\n",
    "removed_count = 0\n",
    "\n",
    "for i, (features, label) in enumerate(zip(X, y)):\n",
    "    if all(np.isfinite(f) for f in features):\n",
    "        X_clean.append(features)\n",
    "        y_clean.append(label)\n",
    "    else:\n",
    "        removed_count += 1\n",
    "        print(f\"  Removed sample {i}: contains inf/nan\")\n",
    "\n",
    "X, y = X_clean, y_clean\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLLECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ Successfully collected: {len(X)} samples\")\n",
    "print(f\"  - Planets: {sum(y)}\")\n",
    "print(f\"  - Non-planets: {len(y) - sum(y)}\")\n",
    "print(f\"  - Removed (inf/nan): {removed_count}\")\n",
    "\n",
    "if failed_targets:\n",
    "    print(f\"\\n✗ Failed to process {len(failed_targets)} targets:\")\n",
    "    for target, category in failed_targets:\n",
    "        print(f\"  - {target} ({category})\")\n",
    "\n",
    "if len(X) < 4:\n",
    "    print(\"\\n⚠ WARNING: Very small dataset! Model may not be reliable.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Dataset ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1c941c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[32m      3\u001b[39m model = RandomForestClassifier(n_estimators=\u001b[32m200\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/expoplanet_detection/.venv3.12/lib/python3.12/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/expoplanet_detection/.venv3.12/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:359\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[32m    372\u001b[39m estimator = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimator)(criterion=\u001b[38;5;28mself\u001b[39m.criterion)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/expoplanet_detection/.venv3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:2919\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2917\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2918\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2919\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2920\u001b[39m     out = X, y\n\u001b[32m   2922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/expoplanet_detection/.venv3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:1314\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1309\u001b[39m         estimator_name = _check_estimator_name(estimator)\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1311\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1312\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1331\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1333\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/expoplanet_detection/.venv3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:1060\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1053\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1054\u001b[39m             msg = (\n\u001b[32m   1055\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1056\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1057\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1058\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mif it contains a single sample.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1059\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array.dtype, \u001b[33m\"\u001b[39m\u001b[33mkind\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array.dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mUSV\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1063\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1064\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnumeric\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1065\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1066\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4e406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model trained on 38 samples with 16 features\n",
      "  Training accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Update feature names to match new feature set (16 features)\n",
    "feature_names = [\n",
    "    'Period',           # 0: Orbital period (days)\n",
    "    'Depth',            # 1: Transit depth\n",
    "    'Duration',         # 2: Transit duration (days)\n",
    "    'SNR',              # 3: Signal-to-noise ratio (SDE)\n",
    "    'SDE_Pass',         # 4: SDE threshold pass (1) or fail (0)\n",
    "    'Rp_Rs',            # 5: Planet-to-star radius ratio\n",
    "    'SNR_Pink',         # 6: SNR accounting for pink noise\n",
    "    'OE_Mismatch',      # 7: TLS odd-even mismatch\n",
    "    'Symmetry',         # 8: Transit symmetry\n",
    "    'Shape_Ratio',      # 9: Ingress/egress ratio\n",
    "    'Depth_Std',        # 10: Depth standard deviation\n",
    "    'OE_Depth_Diff',    # 11: Odd-even depth difference\n",
    "    'OE_Duration_Diff', # 12: Odd-even duration difference\n",
    "    'OE_MAD_Ratio',     # 13: Odd-even MAD ratio\n",
    "    'N_Sectors',        # 14: Number of TESS sectors\n",
    "    'N_Points'          # 15: Total data points\n",
    "]\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(f\"✓ Model trained on {len(X)} samples with {len(feature_names)} features\")\n",
    "print(f\"  Training accuracy: {model.score(X, y):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17be8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        23\n",
      "           1       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "pred = model.predict(X)\n",
    "print(classification_report(y, pred))\n",
    "\n",
    "# ConfusionMatrixDisplay.from_estimator(model, X, y)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2b4b7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_star(target, save_to_csv=True):\n",
    "    \"\"\"Predict whether a target has a planet and optionally save to CSV\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PREDICTION FOR: {target}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features(target)\n",
    "    \n",
    "    if features is None:\n",
    "        print(f\"\\n✗ Could not make prediction for {target} (feature extraction failed)\")\n",
    "        return None\n",
    "    \n",
    "    # Make prediction\n",
    "    features_array = np.array(features).reshape(1, -1)\n",
    "    prediction = model.predict(features_array)[0]\n",
    "    probability = model.predict_proba(features_array)[0][1]  # Probability of being a planet\n",
    "    \n",
    "    result = \"Planet\" if prediction == 1 else \"Non-Planet\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PREDICTION RESULT: {result}\")\n",
    "    print(f\"Planet Probability: {probability:.2%}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Save to CSV if requested\n",
    "    if save_to_csv:\n",
    "        import pandas as pd\n",
    "        from datetime import datetime\n",
    "        \n",
    "        csv_file = 'predictions.csv'\n",
    "        \n",
    "        # Create prediction record\n",
    "        record = {\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'target_name': target,\n",
    "            'prediction': result,\n",
    "            'planet_probability': probability\n",
    "        }\n",
    "        \n",
    "        # Add all features\n",
    "        for i, name in enumerate(feature_names):\n",
    "            record[name.lower().replace(' ', '_')] = features[i]\n",
    "        \n",
    "        # Append to CSV\n",
    "        df_new = pd.DataFrame([record])\n",
    "        \n",
    "        try:\n",
    "            # Read existing file if it exists\n",
    "            if Path(csv_file).exists():\n",
    "                df_existing = pd.read_csv(csv_file)\n",
    "                df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "            else:\n",
    "                df_combined = df_new\n",
    "            \n",
    "            # Save combined data\n",
    "            df_combined.to_csv(csv_file, index=False)\n",
    "            print(f\"✓ Prediction saved to {csv_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to save to CSV: {e}\")\n",
    "    \n",
    "    return {'target': target, 'prediction': result, 'probability': probability, 'features': features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf458fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model on new targets:\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Analyzing: TOI-270\n",
      "============================================================\n",
      "Loading TOI-270 from cache...\n",
      "Cache read failed for TOI-270: The unit 'electron / s' is unrecognized.  It can not be converted to other units.. Re-downloading...\n",
      "Searching for TOI-270 in TESS archive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 9 results\n",
      "  Downloading (attempt 1/3)...\n",
      "  ✓ Successfully downloaded TOI-270\n",
      "  ✓ Lightcurve loaded: 12982 data points\n",
      "  ✓ Detrended: 12982 clean data points\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 36 durations\n",
      "Searching 12982 data points, 1745 periods from 0.602 to 10.139 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1745/1745 periods | 00:05<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 5.66230 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12982/12982 [00:01<00:00, 10075.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ TLS complete: Period=5.66d, Depth=0.9968, SNR=13.69\n",
      "  ✓ Odd-even test: depth_diff=0.000016\n",
      "  ✓ Features extracted successfully\n",
      "✓ Saved to predictions.csv\n",
      "⭐ Likely NOT a planet. Probability of planet: 0.16\n",
      "\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 38846515\n",
      "============================================================\n",
      "Loading TIC 38846515 from cache...\n",
      "Cache read failed for TIC 38846515: The unit 'electron / s' is unrecognized.  It can not be converted to other units.. Re-downloading...\n",
      "Searching for TIC 38846515 in TESS archive...\n",
      "  Found 64 results\n",
      "  Downloading (attempt 1/3)...\n",
      "  ✓ Successfully downloaded TIC 38846515\n",
      "  ✓ Lightcurve loaded: 15871 data points\n",
      "  ✓ Detrended: 15871 clean data points\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 38 durations\n",
      "Searching 15871 data points, 2362 periods from 0.601 to 13.051 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2362/2362 periods | 00:07<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 2.84876 days\n",
      "  ✓ TLS complete: Period=2.85d, Depth=0.9926, SNR=20.13\n",
      "  ✓ Odd-even test: depth_diff=0.000080\n",
      "  ✓ Features extracted successfully\n",
      "✓ Saved to predictions.csv\n",
      "⭐ Likely NOT a planet. Probability of planet: 0.38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'timestamp': '2026-02-05 11:39:18',\n",
       " 'target_name': 'TIC 38846515',\n",
       " 'prediction': 'Non-Planet',\n",
       " 'planet_probability': np.float64(0.375),\n",
       " 'period_days': np.float64(2.8488),\n",
       " 'depth': np.float64(0.992552),\n",
       " 'duration_days': np.float64(0.12267),\n",
       " 'snr': np.float64(20.1298),\n",
       " 'odd_even_diff': np.float64(8e-05)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Testing the model on new targets:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "predict_star(\"TOI-270\")\n",
    "\n",
    "print()\n",
    "\n",
    "predict_star(\"TIC 38846515\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cae048b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Analyzing: TIC 307210830\n",
      "============================================================\n",
      "Loading TIC 307210830 from cache...\n",
      "Cache read failed for TIC 307210830: The unit 'electron / s' is unrecognized.  It can not be converted to other units.. Re-downloading...\n",
      "Searching for TIC 307210830 in TESS archive...\n",
      "  Found 45 results\n",
      "  Downloading (attempt 1/3)...\n",
      "  ✓ Successfully downloaded TIC 307210830\n",
      "  ✓ Lightcurve loaded: 17577 data points\n",
      "  ✓ Available sectors: 0\n",
      "  ✓ Detrended: 17577 clean data points\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 38 durations\n",
      "Searching 17577 data points, 2390 periods from 0.601 to 13.182 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2390/2390 periods | 00:07<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 3.69144 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7642/7642 [00:01<00:00, 6057.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ TLS complete: Period=3.69d, Depth=0.9983, SNR=21.04\n",
      "  ✓ SDE threshold (7.0): PASS (SNR=21.04)\n",
      "  ✓ Model fit: Rp/Rs=0.0370, SNR_pink=10.71\n",
      "  ✓ Shape features: symmetry=0.001230, ratio=0.0208\n",
      "  ✓ Odd-even test: depth_diff=0.000092, dur_diff=0.0184\n",
      "  ✓ Extracted 16 features successfully\n",
      "\n",
      "\n",
      "Extracted 16 features:\n",
      "  Period: 3.69 days\n",
      "  Depth: 0.998312\n",
      "  Duration: 0.0457 days\n",
      "  SNR: 21.04\n",
      "  SDE Pass: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_features = extract_features(\"TIC 307210830\")\n",
    "if test_features:\n",
    "    print(f\"\\n\\nExtracted {len(test_features)} features:\")\n",
    "    print(f\"  Period: {test_features[0]:.2f} days\")\n",
    "    print(f\"  Depth: {test_features[1]:.6f}\")\n",
    "    print(f\"  Duration: {test_features[2]:.4f} days\")\n",
    "    print(f\"  SNR: {test_features[3]:.2f}\")\n",
    "    print(f\"  SDE Pass: {test_features[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e911c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch predictions...\n",
      "============================================================\n",
      "\n",
      "======================================================================\n",
      "PREDICTION FOR: TIC 16740101\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 16740101\n",
      "============================================================\n",
      "Loading TIC 16740101 from cache...\n",
      "Cache read failed for TIC 16740101: The unit 'electron / s' is unrecognized.  It can not be converted to other units.. Re-downloading...\n",
      "Searching for TIC 16740101 in TESS archive...\n",
      "  Found 8 results\n",
      "  Downloading (attempt 1/3)...\n",
      "  ✓ Successfully downloaded TIC 16740101\n",
      "  ✓ Lightcurve loaded: 13816 data points\n",
      "  ✓ Available sectors: 0\n",
      "  ✓ Detrended: 13816 clean data points\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 38 durations\n",
      "Searching 13816 data points, 2443 periods from 0.601 to 13.425 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2443/2443 periods | 00:06<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 1.48108 days\n",
      "  ✓ TLS complete: Period=1.48d, Depth=0.9929, SNR=34.25\n",
      "  ✓ SDE threshold (7.0): PASS (SNR=34.25)\n",
      "  ✓ Model fit: Rp/Rs=0.0759, SNR_pink=110.24\n",
      "  ✓ Shape features: symmetry=0.000865, ratio=0.0103\n",
      "  ✓ Odd-even test: depth_diff=0.000015, dur_diff=0.0222\n",
      "  ✓ Extracted 16 features successfully\n",
      "\n",
      "======================================================================\n",
      "PREDICTION RESULT: Non-Planet\n",
      "Planet Probability: 13.00%\n",
      "======================================================================\n",
      "✓ Prediction saved to predictions.csv\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PREDICTION FOR: TIC 188350478\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 188350478\n",
      "============================================================\n",
      "Searching for TIC 188350478 in TESS archive...\n",
      "  No SPOC data, trying all authors...\n",
      "  ✗ No data found for TIC 188350478\n",
      "✗ Failed to load lightcurve for TIC 188350478\n",
      "\n",
      "✗ Could not make prediction for TIC 188350478 (feature extraction failed)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PREDICTION FOR: TIC 38964113\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 38964113\n",
      "============================================================\n",
      "Searching for TIC 38964113 in TESS archive...\n",
      "  No SPOC data, trying all authors...\n",
      "  ✗ No data found for TIC 38964113\n",
      "✗ Failed to load lightcurve for TIC 38964113\n",
      "\n",
      "✗ Could not make prediction for TIC 38964113 (feature extraction failed)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PREDICTION FOR: TIC 377909730\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Analyzing: TIC 377909730\n",
      "============================================================\n",
      "Loading TIC 377909730 from cache...\n",
      "Cache read failed for TIC 377909730: The unit 'electron / s' is unrecognized.  It can not be converted to other units.. Re-downloading...\n",
      "Searching for TIC 377909730 in TESS archive...\n",
      "  Found 8 results\n",
      "  Downloading (attempt 1/3)...\n",
      "  ✓ Successfully downloaded TIC 377909730\n",
      "  ✓ Lightcurve loaded: 19608 data points\n",
      "  ✓ Available sectors: 0\n",
      "  ✓ Detrended: 19608 clean data points\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 38 durations\n",
      "Searching 19608 data points, 2588 periods from 0.601 to 14.1 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2588/2588 periods | 00:07<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 7.82947 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14208/14208 [00:01<00:00, 8296.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ TLS complete: Period=7.83d, Depth=0.9827, SNR=6.27\n",
      "  ✓ SDE threshold (7.0): FAIL (SNR=6.27)\n",
      "  ✓ Model fit: Rp/Rs=0.1184, SNR_pink=3.20\n",
      "  ✓ Shape features: symmetry=0.044029, ratio=0.0216\n",
      "  ✓ Odd-even test: depth_diff=0.006305, dur_diff=0.0083\n",
      "  ✓ Extracted 16 features successfully\n",
      "\n",
      "======================================================================\n",
      "PREDICTION RESULT: Non-Planet\n",
      "Planet Probability: 16.00%\n",
      "======================================================================\n",
      "✓ Prediction saved to predictions.csv\n",
      "\n",
      "\n",
      "============================================================\n",
      "PREDICTIONS SUMMARY\n",
      "============================================================\n",
      "\n",
      "✓ All predictions saved to: predictions.csv\n"
     ]
    }
   ],
   "source": [
    "test_targets = [\"TIC 16740101\",\"TIC 188350478\",\"TIC 38964113\",\"TIC 377909730\"]\n",
    "\n",
    "print(\"Running batch predictions...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for target in test_targets:\n",
    "    predict_star(target)\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTIONS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n✓ All predictions saved to: predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e353b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations from saved predictions\n",
    "\n",
    "# visualize_predictions('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caa9fe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding a new prediction to test append functionality...\n",
      "\n",
      "======================================================================\n",
      "PREDICTION FOR: Ross 176\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Analyzing: Ross 176\n",
      "============================================================\n",
      "Loading Ross 176 from cache...\n",
      "Cache read failed for Ross 176: The unit 'electron / s' is unrecognized.  It can not be converted to other units.. Re-downloading...\n",
      "Searching for Ross 176 in TESS archive...\n",
      "  Found 5 results\n",
      "  Downloading (attempt 1/3)...\n",
      "  ✓ Successfully downloaded Ross 176\n",
      "  ✓ Lightcurve loaded: 19612 data points\n",
      "  ✓ Available sectors: 0\n",
      "  ✓ Detrended: 19612 clean data points\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 38 durations\n",
      "Searching 19612 data points, 2554 periods from 0.601 to 13.94 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2554/2554 periods | 00:06<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 5.00610 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8716/8716 [00:01<00:00, 7311.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ TLS complete: Period=5.01d, Depth=0.9988, SNR=15.90\n",
      "  ✓ SDE threshold (7.0): PASS (SNR=15.90)\n",
      "  ✓ Model fit: Rp/Rs=0.0313, SNR_pink=4.79\n",
      "  ✓ Shape features: symmetry=0.001843, ratio=0.0044\n",
      "  ✓ Odd-even test: depth_diff=0.000052, dur_diff=0.0204\n",
      "  ✓ Extracted 16 features successfully\n",
      "\n",
      "======================================================================\n",
      "PREDICTION RESULT: Planet\n",
      "Planet Probability: 55.50%\n",
      "======================================================================\n",
      "✓ Prediction saved to predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'target': 'Ross 176',\n",
       " 'prediction': 'Planet',\n",
       " 'probability': np.float64(0.555),\n",
       " 'features': [5.006096565297495,\n",
       "  0.9987868489068944,\n",
       "  0.06250958549172538,\n",
       "  15.90285986821043,\n",
       "  1,\n",
       "  0.031323089671185946,\n",
       "  4.787299780223848,\n",
       "  0.5351055081733053,\n",
       "  0.0018426010063771186,\n",
       "  0.0044444444444444444,\n",
       "  0.0013101594820660433,\n",
       "  5.1960754647972784e-05,\n",
       "  0.020417413455745922,\n",
       "  0.15860091866085455,\n",
       "  0,\n",
       "  19612]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_star(\"Ross 176\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
